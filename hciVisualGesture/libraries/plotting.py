#!/usr/bin/python3
"""
    This module contains the functions to graph the analog signals generated by the aspect ratios of the eyes and mouth.
"""
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.lines import Line2D

# Live graphing function
plt.style.use('ggplot')
def live_plotter(x_vec,y1_data,line,identifier='',pause_time=0.001):
    custom_lines = [Line2D([0], [0], color='r', lw=2),
                Line2D([0], [0], color='b', lw=2),
                Line2D([0], [0], color='g', lw=2)]

    if line==[]:
        # Matplotlib call allowing dynamic tracing
        plt.ion()
        fig = plt.figure('Aspect Ratios',figsize=(5,7))
        ax = fig.add_subplot(1,1,1)
        ax.set_ylim([0,360])
        plt.ylim(0,4)
        ax.legend(custom_lines, ['eye left', 'eye right', 'mouth'],loc='upper left')

        # Create a variable for the line so that we can then update it
        line, = ax.plot(x_vec,y1_data,identifier,alpha=0.8)

        # Update plot tag / title
        plt.ylabel('Aspect ratio')
        plt.xlabel('Frame Number')
        plt.title('Aspect Ratio - EAR and MAR')
        plt.show()
    
    # After creating the figure, axis and line, we just need to update the data in Y
    line.set_ydata(y1_data)
    line.set_xdata(x_vec)

    # Adjust limits if new data goes beyond limits
    if np.min(y1_data)<=line.axes.get_ylim()[0] or np.max(y1_data)>=line.axes.get_ylim()[1]:
        plt.ylim([np.min(y1_data)-np.std(y1_data),np.max(y1_data)+np.std(y1_data)])

    if np.min(x_vec)<=line.axes.get_xlim()[0] or np.max(x_vec)>=line.axes.get_xlim()[1]:
        plt.xlim([np.min(x_vec),np.max(x_vec)])

    # this pauses the data so that the figure / axis can catch up - the amount of pause can be altered above
    plt.pause(pause_time)
    
    return line

# Variables to add each line to the graph
size = 100
frame_counter = 0

x_vec_earL = np.array([val for val in range(-size,0)])
y_vec_earL = np.zeros(len(x_vec_earL))
line_earL = []

x_vec_earR = np.array([val for val in range(-size,0)])
y_vec_earR = np.zeros(len(x_vec_earR))
line_earR = []

x_vec_mar = np.array([val for val in range(-size,0)])
y_vec_mar = np.zeros(len(x_vec_mar))
line_mar = []

# Function that passes the parameters to graph MAR and EAR signals
def graphGestureSigns(ear_left, ear_right, mar_mouth, frameCounter):
    global y_vec_earL, y_vec_earR, y_vec_mar
    global x_vec_earL, x_vec_earR, x_vec_mar
    global line_earL, line_earR, line_mar

    y_vec_earL[-1] = ear_left
    x_vec_earL[-1] = frameCounter
    line_earL = live_plotter(x_vec_earL,y_vec_earL,line_earL,'r')
    y_vec_earL = np.append(y_vec_earL[1:],0.0)
    x_vec_earL = np.append(x_vec_earL[1:],0.0)

    y_vec_earR[-1] = ear_right
    x_vec_earR[-1] = frameCounter
    line_earR = live_plotter(x_vec_earR,y_vec_earR,line_earR,'b')
    y_vec_earR = np.append(y_vec_earR[1:],0.0)
    x_vec_earR = np.append(x_vec_earR[1:],0.0)

    y_vec_mar[-1] = mar_mouth
    x_vec_mar[-1] = frameCounter
    line_mar = live_plotter(x_vec_mar,y_vec_mar,line_mar,'g')
    y_vec_mar = np.append(y_vec_mar[1:],0.0)
    x_vec_mar = np.append(x_vec_mar[1:],0.0)